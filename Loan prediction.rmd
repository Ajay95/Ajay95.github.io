#Consumer Loan Default Prediction 

* Problem Statement 
  * Business Model
* Data Exploration
  * Univariate Analysis
* Model Building
  * Logistic Regression
  * Model building after Data Cleaning
  * Decision Tree

#Problem Statement
Banks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit.

However, giving loan can be a risky business. What if the person or companies, banks have granted loan to are not able to pay it back. It’s not easy to determine the future. However, we can predict the probability of default by a customer by using, Credit Scoring algorithms.

Banks use Credit scores to determine the credit worthiness of the candidate and based on their credit scores they can determine the probabilities of returns.

In this study our goal is to build a model that borrowers can use to help make the best financial decisions. The data is raw, you may have to spend considerable amount of time for validating and cleaning the data.

#Business Model
Banks make profits by interest on credit or loans they have provided to their customers for their needs. Banks have to be careful about whom to grant a loan, as the margin on the interest is quite low but the loss associated with defaulting the loan can be tough to recover. Reducing the probability of a loan default is a priority for a bank. To reduce such cases, banks check credit worthiness of the borrower before giving out any loan. Banks build a model to decide weather a customer is a good candidate for loan or not using information provided by customer an historical loan data which helps banks to find the probability of any future default. Using the data given to us about the borrowers and we will try to build a model which can predict whether a borrower will be able to pay loan or default the loan.

#Data Exploration
Once we have the dataset and metadata, understanding metadata thoroughly is a crucial step. Exploration helps us understand all the variables throughly which is necessary to understand relation between input and predictive variables. Exploration also provides a vague understanding of what’s going on with the dataset.

First step is to import the data set to R. Formats like .csv,.xlsx etc are the common data formats used by data scientists or analysts. Use suitable function to import it into R. In our case data set is in csv format. So we use the function ‘read.csv()’ to import the data set.

    train_data<-read.csv("D:\\Datasets\\Give me some Credit\\cs-training.csv")

We are assigning the data to the variable ‘train_data’

See the dimension of the data.

    dim(train_data)
Output

    ## [1] 150000     12

 we have 12 variables, and 150000 observations.

    names(train_data)
Output

      [1] "Sr_No"                               
      [2] "SeriousDlqin2yrs"                    
      [3] "RevolvingUtilizationOfUnsecuredLines"
      [4] "age"                                 
      [5] "NumberOfTime30.59DaysPastDueNotWorse"
      [6] "DebtRatio"                           
      [7] "MonthlyIncome"                       
      [8] "NumberOfOpenCreditLinesAndLoans"     
      [9] "NumberOfTimes90DaysLate"             
     [10] "NumberRealEstateLoansOrLines"        
     [11] "NumberOfTime60.89DaysPastDueNotWorse"
     [12] "NumberOfDependents"

The above command gets the name of the columns in the dataset

    str(train_data)
Output

     'data.frame':    150000 obs. of  12 variables:
      
    $ Sr_No                               : int  1 2 3 4 5 6 7 8 9 10 ...
    $ SeriousDlqin2yrs                    : int  1 0 0 0 0 0 0 0 0 0 ...
    $ RevolvingUtilizationOfUnsecuredLines: num  0.766 0.957 0.658 0.234 0.907 ...
    $ age                                 : int  45 40 38 30 49 74 57 39 27 57 ...
    $ NumberOfTime30.59DaysPastDueNotWorse: int  2 0 1 0 1 0 0 0 0 0 ...
    $ DebtRatio                           : num  0.803 0.1219 0.0851 0.036 0.0249 ...
    $ MonthlyIncome                       : int  9120 2600 3042 3300 63588 3500 NA 3500 NA 23684 ...
    $ NumberOfOpenCreditLinesAndLoans     : int  13 4 2 5 7 3 8 8 2 9 ...
    $ NumberOfTimes90DaysLate             : int  0 0 1 0 0 0 0 0 0 0 ...
    $ NumberRealEstateLoansOrLines        : int  6 0 0 0 1 1 3 0 0 4 ...
    $ NumberOfTime60.89DaysPastDueNotWorse: int  0 0 0 0 0 0 0 0 0 0 ...
    $ NumberOfDependents                  : int  2 1 0 0 0 1 0 0 NA 2 ...

In our data some variables are integers and some are numeric. Sometimes, categorical variables are given a integer value for each category.

    summary(train_data)
Output

      Sr_No        SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines
    Min.   :     1   Min.   :0.00000   Min.   :    0.00                    
    1st Qu.: 37501   1st Qu.:0.00000   1st Qu.:    0.03                    
    Median : 75001   Median :0.00000   Median :    0.15
    Mean   : 75001   Mean   :0.06684   Mean   :    6.05                    
    3rd Qu.:112500   3rd Qu.:0.00000   3rd Qu.:    0.56                    
    Max.   :150000   Max.   :1.00000   Max.   :50708.00                    
                                                                         
       age                NumberOfTime30.59DaysPastDueNotWorse   DebtRatio       
    Min.   :  0.0   Min.   : 0.000                             Min.   :  0.0  
    1st Qu.:  41.0   1st Qu.: 0.000                            1st Qu.:  0.2
    Median : 52.0   Median : 0.000                             Median : 0.4  
    Mean   : 52.3   Mean   : 0.421                             Mean   : 353.0  
    3rd Qu.: 63.0   3rd Qu.: 0.000                             3rd Qu.: 0.9  
    Max.   :109.0   Max.   :98.000                             Max.   :329664.0  
                                                                         
    MonthlyIncome     NumberOfOpenCreditLinesAndLoans NumberOfTimes90DaysLate
    Min.   :     0   Min.   : 0.000                  Min.   : 0.000         
    1st Qu.:   3400   1st Qu.: 5.000                  1st Qu.: 0.000         
    Median :   5400   Median : 8.000                  Median : 0.000         
    Mean   :   6670   Mean   : 8.453                  Mean   : 0.266         
    3rd Qu.:   8249   3rd Qu.:11.000                  3rd Qu.: 0.000         
    Max.   :3008750   Max.   :58.000                  Max.   :98.000         
    NA's   :29731                                                            
    NumberRealEstateLoansOrLines   NumberOfTime60.89DaysPastDueNotWorse
    Min.   : 0.000               Min.   : 0.0000                     
    1st Qu.: 0.000               1st Qu.: 0.0000                     
    Median : 1.000               Median : 0.0000                     
    Mean   : 1.018               Mean   : 0.2404                     
    3rd Qu.: 2.000               3rd Qu.: 0.0000                     
    Max.   :54.000               Max.   :98.0000                     
                                                                   
    NumberOfDependents
    Min.   : 0.000    
    1st Qu.: 0.000    
    Median : 0.000    
    Mean   : 0.757    
    3rd Qu.: 1.000    
    Max.   :20.000    
    NA's   :3924

Summary will show the minimum,maximum,mean,median,1st quartile,3rd quartile of all the variables in the data set. It also shows whether any variable has NA(missing) values. In our data, variables ‘MonthlyIncome’ and ‘NumberOfDependents’ have NA values. Summary gives mean of variables after excluding NA values.

Missing values aren’t always represented by ‘NA’. We can also find ‘?’,‘-’,‘.’ etc as denotation for missing values. Summary and frequency table of the variable are best ways to find the presence of missing values. Frequency table allows us to find any notation used for missing value. Once we know the representation of the missing values, we can find number of missing values in each variable and move toward missing value treatment.

##Univariate Analysis
In univariate analysis, we validate one variable at a time. This allow us know what each variable signifies and how it is related to predicting variable. Method to perform Univariate analysis depends on weather the variable is continuous or categorical. -1. Continuous : Central tendency(mean, median, mode etc.)and spread of the variable or dispersion(range, quartile, variance, skewness-kurtosis etc.) is the basic way to deal with continuous variables. These can be measured using various statistical metrics. Boxplot can be used for visulisation. -2. Categorical : Frequency tables are used for analysing categorical variables. Count of each variable and/or percentage count helps us understand categorical data. Bar charts can be used for visulisation.

This also helps us identifying any missing values, outliers etc. Once we sanitize the variable data it can be used for final model building.
> ##SeriousDlqin2yrs

Variable ‘SeriousDlqin2yrs’ tells whether a person experienced 90 days past due delinquency or worse. This is the target variable which we have to predict. Being a binary variable, value is either 1(yes) or 0(no).
  
    summary(train_data$SeriousDlqin2yrs)

Output
 
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    0.00000 0.00000 0.00000 0.06684 0.00000 1.00000

Since it is a yes/no type variable we will get frequency table for better understanding of the variable.
    
    count<-table(train_data$SeriousDlqin2yrs)
    count
Output

     0      1 
    139974  10026

Out of 150000 only 10026 are bad accounts, which shows we might be dealing with ‘imbalanced data’

> #RevolvingUtilizationOfUnsecuredLines

This variable represents total balance on credit cards and personal lines of credit, except, real estate and no-installment debt like car loans divided by the sum of credit limits. As this variable is a ratio, the value must be between 0 and 1.
    
    summary(train_data$RevolvingUtilizationOfUnsecuredLines)
Output
    
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
    0.00     0.03     0.15     6.05     0.56 50710.00

The mean is 6.05, which is greater than 1.This variable seem to have some faulty values. The maximum value is 50710 which is way too high.

Lets look at percentiles to know from where it is exceeding 1.

    quantile(train_data$RevolvingUtilizationOfUnsecuredLines,c(0.9,0.91,0.95,0.96,0.97,0.975,0.98,0.99,1))

Output

              90%          91%          95%            96%          97% 
    9.812777e-01 9.999999e-01 9.999999e-01 9.999999e-01 9.999999e-01 
        97.5%          98%          99%         100% 
    9.999999e-01 1.006199e+00 1.092956e+00 5.070800e+04
We can see that there are outliers present in the variable.

>#age

It specifies age of the barrower in years. Its’ an integer. lets see the summary of age

    summary(train_data$age)
Output

     Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
     0.0    41.0    52.0    52.3    63.0   109.0
Minimum age is 0, which is not practical. Maximum age is 109 which is ok. Mean and median are very close which indicates outliers may not be present.

Lets see the percentile distribution.

    quantile(train_data$age,c(0,0.01,0.03,0.05,0.07,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1))

Output

    0%   1%   3%   5%   7%   9%  10%  20%  30%  40%  50%  60%  70%  80%  90% 
    0   24   27   29   30   32   33   39   44   48       52   56   61   65   72 
     100% 
     109

Check whether any barrower age is less than 20 as banks avoid giving credit to teenagers.

    sum(train_data$age<20)

Ouput

     [1] 1

There is only one borrower whose age is less than 20. That borrower is of minimum age 0. This is not possible as banks can not give credit to unborn.

>#NumberOfTime30.59DaysPastDueNotWorse

This variable shows number of times a borrower has been between 30-59 days past due, but no worse in the last 2 years.
    
    str(train_data$NumberOfTime30.59DaysPastDueNotWorse)

Output
    
    int [1:150000] 2 0 1 0 1 0 0 0 0 0 ...

    

Summary of NumberOfTime30.59DaysPastDueNotWorse

    summary(train_data$NumberOfTime30.59DaysPastDueNotWorse)

t is an integer variable. Minimum value is zero, median is also zero. Mean is 0.421  and maximum value is 98. These give indication of presence of outliers.

Check the percentile distribution to know the presence of outliers.

    quantile(train_data$NumberOfTime30.59DaysPastDueNotWorse,c(0,0.01,0.03,0.05,0.07,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.85,0.9,0.95,1))

Output

    0%   1%   3%   5%   7%   9%  10%  20%  30%  40%  50%  60%  70%  80%  85% 
    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1 
    90%  95% 100% 
    1    2   98
100 percentile is 98, which is an outlier.

This variable range is from 0 to 98. It takes only integers values. Lets see it’s frequency distribution.
   
     freq_table<-table(train_data$NumberOfTime30.59DaysPastDueNotWorse)
    freq_table
Output
 
     0      1      2      3      4      5      6      7      8      9 
    126018  16033   4598   1754    747    342    140     54     25     12 
     10     11     12     13     96     98 
      4      1      2      1      5    264

This variables has values from 0 to 13 and 96,98. Last two must be the outliers.

>#DebtRatio

Debt ratio is obtained by dividing Monthly debt payments, alimony, living costs by monthly gross income.

    str(train_data$DebtRatio)
Output
  
    num [1:150000] 0.803 0.1219 0.0851 0.036 0.0249 ...    

Summary 
    
    summary(train_data$DebtRatio)
Output
    
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
     0.0      0.2      0.4    353.0      0.9 329700.0


Usually debt ratio should be between 0 to 1. Somtimes it can exceed 1, if a person spends more than his income. Here its minimum is 0, mean is 353, median is 0.4. This indicates presence of outliers. Maximum value is 329700, which is not possible.

Exploring further by looking at percentile distribution

    quantile(train_data$DebtRatio,c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.75,0.76,0.78,0.8,0.85,0.9,0.95,1))

Output

     10%          20%          30%          40%          50% 
    3.087398e-02 1.337729e-01 2.136969e-01 2.874603e-01 3.665078e-01 
         60%          70%          75%          76%          78% 
    4.675064e-01 6.491891e-01 8.682538e-01 9.511839e-01 1.275069e+00 
          80%          85%          90%          95%         100% 
    4.000000e+00 2.691500e+02 1.267000e+03 2.449000e+03 3.296640e+05

We can see upto 76 percentile DebtRatio is less than 1.


There are outlers present in the variable DebtRatio. We have to filter them before we use the data for model building.

>#MonthlyIncome

This variable represents monthly income of the borrower.
   
    str(train_data$MonthlyIncome)
Output

    int [1:150000] 9120 2600 3042 3300 63588 3500 NA 3500 NA 23684 ...
Summary

    summary(train_data$MonthlyIncome)

    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
     0    3400    5400    6670    8249 3009000   29731

This is an integer variable and all the missing values represented by ‘NA’.
Its minimum value is 0, which is practically impossible. Mean is 6670 and median is 5400 without considering NA values. Maximum value is 3009000.

We have to treat missing values first before going further into this variable analysis.


>#NumberOfOpenCreditLinesAndLoans

This variable indicates number of open loans (an installment loan such as car loan or mortgage) and lines of credit (such as credit cards).
    
    str(train_data$NumberOfOpenCreditLinesAndLoans)

Summary
    
    summary(train_data$NumberOfOpenCreditLinesAndLoans)

Output

    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    0.000   5.000   8.000   8.453  11.000  58.000

It is an integer variable. Its minimum value is 0,maximum value is 58. Its mean is 8.543, median is 8. Mean and median are close, so outliers may not be present.

Lets see percentile distribution to know the outliers presence.


    quantile(train_data$NumberOfOpenCreditLinesAndLoans,c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.93,0.95,0.97,0.98,0.99,0.995,1))

Output

     10%   20%   30%   40%   50%   60%   70%   80%   90%   93%   95%   97% 
    3     4     5     6     8     9    10    12    15    17    18    20 
     98%   99% 99.5%  100% 
     22    24    27    58

Highest value is 58 which seems normal.

>#NumberOfTimes90DaysLate


This variable represents number of times borrower has been 90 days or more past due.
    
    str(train_data$NumberOfTimes90DaysLate)
Output

    int [1:150000] 0 0 1 0 0 0 0 0 0 0 ...
Summary

    summary(train_data$NumberOfTimes90DaysLate)

Output

     Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
     0.000   0.000   0.000   0.266   0.000  98.000

It is an integer variable. Minimum value is zero, median is also zero. Mean is 0.266 and maximum value is 98. These give indication of presence of outliers.

Check the percentile distribution to know the presence of outliers.



    quantile(train_data$NumberOfTimes90DaysLate,c(0,0.01,0.03,0.05,0.07,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.85,0.9,0.95,0.97,0.99,1))


Output

     0%   1%   3%   5%   7%   9%  10%  20%  30%  40%  50%  60%  70%  80%  85% 
    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 
    90%  95%  97%  99% 100% 
    0    1    1    3   98

100 percentile is 98, which is an outlier
.

Variable ranges from 0 to 98. It takes only integers values. Lets see it’s frequency distribution.


    freq_table<-table(train_data$NumberOfTimes90DaysLate)
    freq_table


Output


      0      1      2      3      4      5      6      7      8      9 
    141662   5243   1555    667    291    131     80     38     21     19 
     10     11     12     13     14     15     17     96     98 
      8      5      2      4      2      2      1      5    264



>#NumberRealEstateLoansOrLines

This variable shows number of mortgage and real estate loans taken by the barrower including home equity lines of credit.


    str(train_data$NumberRealEstateLoansOrLines)

Output

    int [1:150000] 6 0 0 0 1 1 3 0 0 4 ...
Summary


    summary(train_data$NumberRealEstateLoansOrLines)
Output

     Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
     0.000   0.000   1.000   1.018   2.000  54.000

It is an integer variable. Minimum value is zero, median is one. Mean is 1.018 and maximum value is 54. Mean and Median are close so, there may not be outliers in this variable.

Check the percentile distribution to know the presence of outliers.


    quantile(train_data$NumberRealEstateLoansOrLines,c(0,0.01,0.03,0.05,0.07,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.85,0.9,0.95,0.97,0.99,1))

100 percentile is at 54, which is a possible value for this variable.

This variable ranges from 0 to 54. It takes only integers values. Let’s see frequency distribution of this variable.



    freq_table<-table(train_data$NumberRealEstateLoansOrLines)
    freq_table

Output

     0     1     2     3     4     5     6     7     8     9    10    11 
    56188 52338 31522  6300  2170   689   320   171    93    78    37    23 
    12    13    14    15    16    17    18    19    20    21    23    25 
    18    15     7     7     4     4     2     2     2     1     2     3 
    26    29    32    54 
    1     1     1     1




This variables has values from 0 to 21 and 23,25,26,29,32,54

There seem to be no outliers in this variable



>#NumberOfTime60.89DaysPastDueNotWorse



It shows number of times borrower has been 60 to 89 days past due but no worse in the last 2 years.


    str(train_data$NumberOfTime60.89DaysPastDueNotWorse)


Output

    int [1:150000] 0 0 0 0 0 0 0 0 0 0 ...

Summary
    
    summary(train_data$NumberOfTime60.89DaysPastDueNotWorse)


Output
    
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    0.0000  0.0000  0.0000  0.2404  0.0000 98.0000

It is an integer variable. Minimum value is zero, median is 0. Mean is 0.2404 and maximum value is 98. These give indication of presence of outliers.

Check the percentile distribution to know the presence of outliers.


    quantile(train_data$NumberOfTime60.89DaysPastDueNotWorse,c(0,0.01,0.03,0.05,0.07,0.09,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.85,0.9,0.95,0.97,0.99,1))

Output

    0%   1%   3%   5%   7%   9%  10%  20%  30%  40%  50%  60%  70%  80%  85% 
    0    0    0    0    0    0    0    0    0    0      0    0    0    0    0 
    90%  95%  97%  99% 100% 
    0    1    1    2   98
   
100 percentile is 98, which is an outlier.

This variable range is from 0 to 98.It takes only integers values. Lets see it’s frequency distribution.


    freq_table<-table(train_data$NumberOfTime60.89DaysPastDueNotWorse)
    freq_table

Output

     0      1      2      3      4      5      6      7      8      9 
     142396   5731   1118    318    105     34     16        9      2      1 
     11     96     98 
     1      5    264

This variables has values from 0 to 9 and 11, 96, 98. Last two must be outliers.


>#NumberOfDependents


It represents number of dependents in the family of borrower excluding the borrower.

    str(train_data$NumberOfDependents)
Output

    int [1:150000] 2 1 0 0 0 1 0 0 NA 2 ...


Summary

    summary(train_data$NumberOfDependents)
Output

    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
    0.000   0.000   0.000   0.757   1.000  20.000    3924
It is an integer variable. It has quite a lot missing values represented by ‘NA’.
Its minimum value is 0. Mean is 0.757 and median is 0 without considering NA values. Maximum value is 20.

Missing values need to be treated first before going further into this variable analysis.

We will build a model using uncleaned data and later clean the data.

>##Model Building
Since the predictor variable (SeriousDlqin2yrs) is YES or NO type (Binary), logistic regression model seem to be a fair apporach.

The dataset contains training data set but no test data. We will divide our data set into two parts, first 125000 rows for training and remaining 25000 rows for testing.

First we build model without cleaning data, then we replace missing values, outliers with mean and build new model. We will check the accuracy of the model in both cases. If accuracy is increases after treating data then we will keep the model otherwise we will discard it. We do this for other models as well, we will take the model which gives better results.

>###Logistic Regression

Splitting dataset into training and testing

  
    train_dataset<-train_data[1:125000,]
    test_dataset<-train_data[125001:150000,]

Next we will build the model using train_dataset


    logmodel<- glm(SeriousDlqin2yrs ~ RevolvingUtilizationOfUnsecuredLines + age + NumberOfTime30.59DaysPastDueNotWorse + DebtRatio +   MonthlyIncome+NumberOfOpenCreditLinesAndLoans+NumberOfTimes90DaysLate+ NumberRealEstateLoansOrLines+ NumberOfTime60.89DaysPastDueNotWorse+NumberOfDependents, data = train_dataset, family = "binomial")


Using this model we will predict the output of test_dataset


    
    predicted_values<-ifelse(predict(logmodel,test_dataset)>0.5,1,0)
  


Our threshold (Decision Boundary) is taken 0.5. Any value greater than 0.5 is taken as 1, otherwise 0.

As we have predicted the values, we will use confusion matrix to calculate the accuracy of the model.


    conf_matrix<-table(test_dataset[,2],predicted_values)
    conf_matrix


Output

    predicted_values
         0     1
       0 18576    27
       1  1402    41


Here columns are predicted values, rows are actual values.

Accuracy is total true predictions divided by total predictions. We will also find sensitivity and specificity. 

Sensitivity is

 
    (true positives/(true positives+false negatives)) 


Specificity is
 
     (true negatives/(true negatives+false positivies))




Accuracy

    accuracy<-(conf_matrix[1,1]+conf_matrix[2,2])/sum(conf_matrix)
    accuracy


 
Output


    [1] 0.928714



Specificity

    specificity<-conf_matrix[2,2]/(conf_matrix[2,2]+conf_matrix[2,1])  
    sensitivity<-conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[1,2])
    specificity
    sensitivity

Output
    
    0.02841303
    0.9985486


Our model accuracy with uncleaned data is 0.928. Specificity is 0.028, which being quite low our model needs improvements.

In next step we will apply missing values treatment on the data, replacing the missing values with their means and will build new model.


>###Missing Values Treatment

MonthlyIncome and NumberOfDependents have missing values. After creating a new dataset we will replace missing values in above columns by their column mean values. ###MonthlyIncome In MonthlyIncome missing values are of 19.82%. We create a new column NA_MonthlyIncome which indicates whether the original value in MonthlyIncome was NA or not.


    train_data1<-train_data
    c1<-c(1:150000)[is.na(train_data[,7])]
    train_data1[c1,7]<-6670
  
    train_data1$NA_MonthlyIncome<-is.na(train_data[,7])

###NumberOfDependents In column NumberOfDependents missing values are of only 2.616%, we can just replace missing values by mean of remaining values without creating a new column


    c1<-c(1:150000)[is.na(train_data[,12])]
    train_data1[c1,12]<-0.757


Splitting new dataset train_data into training and testing datasets.


    train_dataset1<-train_data1[1:125000,]
    test_dataset1<-train_data1[125001:150000,]


Next task is to build a new model, predicting values in testing daata and finding the accuracy.



    logmodel1 <- glm(SeriousDlqin2yrs ~ RevolvingUtilizationOfUnsecuredLines + age + NumberOfTime30.59DaysPastDueNotWorse + DebtRatio +   MonthlyIncome+NumberOfOpenCreditLinesAndLoans+NumberOfTimes90DaysLate+ NumberRealEstateLoansOrLines+ NumberOfTime60.89DaysPastDueNotWorse+NumberOfDependents+NA_MonthlyIncome, data = train_dataset1, family = "binomial")
    

    predicted_values1<-ifelse(predict(logmodel1,test_dataset1)>0.5,1,0)
    conf_matrix<-table(test_dataset1[,2],predicted_values1)
    conf_matrix
 
The confusion Matrix

     predicted_values1
            0     1
       0 23234    34
       1  1679    53

The accuracy


    accuracy<-(conf_matrix[1,1]+conf_matrix[2,2])/sum(conf_matrix)
    accuracy


    [1] 0.93148


Specficity
    
    specificity<-conf_matrix[2,2]/(conf_matrix[2,2]+conf_matrix[2,1])
    specificity

Output

    0.93148

Sensitivity

    sensitivity<-conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[1,2])
    sensitivity

Output

    0.9985388

We can see model accuracy has been improved to 0.931after missing values treatment. But we still need some improvement in specificity.

In next step we will treat outliers.


>###Outliers Removal



We will create new dataset train_data2 using train_data1 in which we will replace the outliers with mean of that column.


    train_data2<-train_data1





####RevolvingUtilizationOfUnsecuredLines


RevolvingUtilizationOfUnsecuredLines is a ratio, any value greater than 1 must be outlier. Since outliers percentage is less than 10, We will replace outliers with mean of remaining data.


    remain_mean<-mean(train_data1$RevolvingUtilizationOfUnsecuredLines[c(1:150000)[train_data1$RevolvingUtilizationOfUnsecuredLines<=1]])

    train_data2[c(1:150000)[train_data1$RevolvingUtilizationOfUnsecuredLines>1],3]<-remain_mean



####age

Only outlying value in variable ‘Age’ is zero, we replace it with mean of remaining values.




    remain_mean<-mean(train_data1$age[c(1:150000)[train_data1$age>0]])

    train_data2[c(1:150000)[train_data1$age==0],4]<-remain_mean


####NumberOfTime30.59DaysPastDueNotWorse

NumberOfTime30.59DaysPastDueNotWorse has values 96,98 as outliers which are of less than 10%. We can treat the outliers based on the related variable ‘SeriousDlqin2yrs’. ‘NumberOfTime30.59DaysPastDueNotWorse’ is directly related to ‘SeriousDlqin2yrs’. We create a frequency table between SeriousDlqin2yrs and NumberOfTime30.59DaysPastDueNotWorse.




    cross_table<-table(train_data1$NumberOfTime30.59DaysPastDueNotWorse,train_data1$SeriousDlqin2yrs)
    cross_table


Output of the table


          0      1
    0  120977   5041
    1   13624   2409
    2    3379   1219
    3    1136    618
    4     429    318
    5     188    154
    6      66     74
    7      26     28
    8      17      8
    9       8      4
    10      1      3
    11      0      1
    12      1      1
    13      0      1
    96      1      4
    98    121    143


For all the values in NumberOfTime30.59DaysPastDueNotWorse find the percentage of 0’s in SeriousDlqin2yrs. As both variables are related, We replace 96, 98 with the values whose 0’s percentage is same as former values.



    percent=0
    for(i in 1:nrow(cross_table)){
  percent[i]=(cross_table[i,1]/(cross_table[i,1]+cross_table[i,2]))*100 
}
    percent


    95.99978 84.97474 73.48847 64.76625 57.42972 54.97076 47.14286
    48.14815 68.00000 66.66667 25.00000  0.00000 50.00000  0.00000
    20.00000 45.83333

We can see that for 98, 6 values have nearly same percent(45.83) and there are only 5 values of 96 in the variable. By this deduction we can replace both 98 and 96 with 6.


    train_data2[c(1:150000)[train_data1[,5]>13],5]<-6




###DebtRatio


For DebtRatio we will take anything greater than 1 as outlier and replace them with remaining values mean. As outliers percentage is 23.4%, we crete a new row Outlier_DebtRatio to indicate whether that value is outlier and replaced or not.



    remain_mean<-mean(train_data1$DebtRatio[c(1:150000)[train_data1$DebtRatio<1]])
    train_data2$Outlier_DebtRatio<-train_data1$DebtRatio>1
    train_data2[c(1:150000)[train_data1$DebtRatio>1],6]<-remain_mean


###MonthlyIncome






In MonthlyIncome we replaced missing values with mean. 

Observing the percentile distribution that we drew earlier we will take upper limit as 50000 and lower limit as 1000. Anything out of this range will be considered as outlier. Let’s see total number of outliers


    sum(train_data1$MonthlyIncome>50000)
Output

    301
    sum(train_data1$MonthlyIncome<1000)

Output
   
    4428

Outliers are of 3.152%.We replace outliers with median of remaining values.


    median<-median(c(1:150000)[train_data1$MonthlyIncome<50000&train_data1$MonthlyIncome>1000])
    median

Output
 
    75010.5

We insert this mean value in place of the outliers

    train_data2$MonthlyIncome[c(1:150000)[train_data1$MonthlyIncome<50000&train_data1$MonthlyIncome>1000]]<-median


###NumberOfTimes90DaysLate



Outlier in NumberOfTimes90DaysLate can be treated same same as NumberOfTime30.59DaysPastDueNotWorse. Outliers are 96,98.



    cross_table<-table(train_data1$NumberOfTimes90DaysLate,train_data1$SeriousDlqin2yrs)
    cross_table


Output

         0      1
    0  135108   6554
    1    3478   1765
    2     779    776
    3     282    385
    4      96    195
    5      48     83
    6      32     48
    7       7     31
    8       6     15
    9       5     14
    10      3      5
    11      2      3
    12      1      1
    13      2      2
    14      1      1
    15      2      0
    17      0      1
    96      1      4
    98    121    143


We find  their ratios

    percent=0
    for(i in 1:nrow(cross_table)){
         percent[i]=    (cross_table[i,1]/(cross_table[i,1]+cross_table[i,2]))*100 
    }
    percent



Output

    [1]  95.37349  66.33607  50.09646  42.27886  32.98969  36.64122  40.00000
         18.42105  28.57143  26.31579  37.50000  40.00000  50.00000  50.00000
         50.00000 100.00000   0.00000  20.00000  45.83333



For values 98, 3 has close percentage. We replace both 96 and 98 with 3.


    train_data2[c(1:150000)[train_data1[,9]>17],9]<-3



###NumberOfTime60.89DaysPastDueNotWorse


This variable again has 96, 98 as outliers and can be treated as NumberOfTime30.59DaysPastDueNotWorse.


    cross_table<-table(train_data1$NumberOfTime60.89DaysPastDueNotWorse,train_data1$SeriousDlqin2yrs)
    cross_table

Output

        0      1
 
    0  135140   7256
    1    3954   1777
    2     557    561
    3     138    180
    4      40     65
    5      13     21
    6       4     12
    7       4      5
    8       1      1
    9       1      0
    11      0      1
    96      1      4
    98    121    143

We find their ratios

    percent=0
    for(i in 1:nrow(cross_table)){
      percent[i]=(cross_table[i,1]/(cross_table[i,1]+cross_table[i,2]))*100 
    }
    percent

Output

      94.90435  68.99319  49.82111  43.39623  38.09524  38.23529  25.00000
      44.44444  50.00000 100.00000   0.00000  20.00000  45.83333


Values 98, 7 has close percentage. We will replace both 96 and 98 with 7


    train_data2[c(1:150000)[train_data1[,11]>17],11]<-7

As all the outliers and missing values are cleaned, we can save and move further to create model with dataset ‘train_data2’


.

    write.csv(train_data2,".../cleaned-data.csv")



##Model building after Data Cleaning
After Outlier Treatment, We can build a new model again using new data set. We divide data set for training and testing.


    train_dataset2<-train_data2[1:125000,]
    test_dataset2<-train_data2[125001:150000,]

Creating the model, predecting the values on test data and finding accuracy:


    logmodel2 <- glm(SeriousDlqin2yrs ~ RevolvingUtilizationOfUnsecuredLines + age + NumberOfTime30.59DaysPastDueNotWorse + DebtRatio +   MonthlyIncome+NumberOfOpenCreditLinesAndLoans+NumberOfTimes90DaysLate+ NumberRealEstateLoansOrLines+ NumberOfTime60.89DaysPastDueNotWorse+NumberOfDependents+ NA_MonthlyIncome + Outlier_DebtRatio, data = train_dataset2, family = "binomial")

    predicted_values<-ifelse(predict(logmodel2,test_dataset2)>0.5,1,0)
    conf_matrix<-table(test_dataset2[,2],predicted_values)
    conf_matrix

Output

    predicted_values
         0     1
    0 23163   105
    1  1566   166

    accuracy<-(conf_matrix[1,1]+conf_matrix[2,2])/sum(conf_matrix)
    specificity<-conf_matrix[2,2]/(conf_matrix[2,2]+conf_matrix[2,1])
    sensitivity<-conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[1,2])

    accuracy
    specificity
    sensitivity


Output

    0.93316
    0.09584296
    0.9954874

Accuracy of the model is 0.933 which is greater than that of previous model. Specificity is 0.0984, which is still very low. We need to implement another way to improve Specificity.


##Accuracy paradox

In our data there are way more number of positive cases(0) than negative cases(1).

    table(train_data[,2])


Output
      
     0      1 
    139974  10026


Ratio of zeros to ones is nearly 14:1 which indicates our data is imbalanced data. Imbalanced data typically refers to a problem with classification where the classes are not represented equally.

In our case models are giving more than 90% accuracy, but the accuracy is reflecting just the underlying class distribution. If we notice the confusion matrix, we can see that accuracy is mostly driven by ’0’s. Even though it is misclassifying most of the ’1’s we are getting high accuracy. This situation is called Accuracy Paradox.

In our case if we classify a good person as bad, bank doesn’t issue loan to him and loses interest. If we classify bad loan candidate as good, bank issues loan to him but he won’t be able to pay the loan. Bank will face huge loss in later case. It is ok to calssify a good one as bad, but we should not classify bad one as good. Which basically means specificity should be as high as possible in our case. With the low model specificity of 0.155 we need to take measures to increase specificity. How can we do that?

One way to tackle the issue is using Averaging models imbalance and increase the specificity.


###Averaging Models

The ratio of ones to zeros is 1:14. We divide data into two groups with ones and zeros.


    zeros_data<-train_data2[-c(1:150000)[train_data2[,2]==1],]
    ones_data<-train_data2[-c(1:150000)[train_data2[,2]==0],]



Next we divide the zeros_data into 14 clusters using K-means clustering. Each time clustering changes when we run the code. So to get same clusters each time we write the line ‘set.seed(42)’ in the code.

    set.seed(42)
    fit <- kmeans(zeros_data, 14)
    zeros_clust<-data.frame(zeros_data, fit$cluster)
    table(zeros_clust[,15])

     1     2     3     4     5     6     7     8     9    10    11    12 
     9733  9725 15115  1469  1514 15188  9804 15130 13949 11901 15061 10074 
    13    14 
    9746  1565

zeros_clust has new column indicating cluster number. Above table shows number of elements in each cluster.

Next we build 14 models using each cluster and ones_data. ones_data will be used in all clusters , thus making most of the training sets a balanced ones in building models.

    models_list <- vector(mode="list", length=14)
    for(j in 1:14){
    models_list[[j]]<-ctree(SeriousDlqin2yrs ~ RevolvingUtilizationOfUnsecuredLines + age + NumberOfTime30.59DaysPastDueNotWorse + DebtRatio +   MonthlyIncome+NumberOfOpenCreditLinesAndLoans+NumberOfTimes90DaysLate+ NumberRealEstateLoansOrLines+ NumberOfTime60.89DaysPastDueNotWorse+NumberOfDependents+ NA_MonthlyIncome + Outlier_DebtRatio, data = rbind(ones_data,zeros_data[c(1:nrow(zeros_clust))[zeros_clust[,15]==j],]))}
Accuracy,specificity,sensitivity for all these models


    specificity=0
    accuracy=0
    sensitivity=0
    predict_values<-data.frame(matrix(0, ncol = 14, nrow = 150000))
    for(j in 1:14){
    predict_values[,j]<-predict(models_list[[j]], newdata=train_data2)
    predicted_values<-ifelse(predict_values[,j]>0.5,1,0)
    conf_matrix1<-table(train_data2[,2],predicted_values)

    accuracy[j]<-(conf_matrix1[1,1]+conf_matrix1[2,2])/sum(conf_matrix1)
    specificity[j]<-conf_matrix1[2,2]/(conf_matrix1[2,2]+conf_matrix1[2,1])
    sensitivity[j]<-conf_matrix1[1,1]/(conf_matrix1[1,1]+conf_matrix1[1,2])

    }
    accuracy
    sensitivity
    specificity

Output

    [1] 0.7571667 0.7607400 0.8502800 0.0947400 0.0949000 0.8513067 0.7829267
    [8] 0.8454733 0.8068267 0.7847400 0.8475600 0.7821267 0.7776200 0.0950200

    [1] 0.75431151 0.75855516 0.86400331 0.03087002 0.03110578 0.86520354
    [7] 0.78372412 0.85819509 0.81168646 0.78580308 0.86078843 0.78298113
    [13] 0.77784446 0.03129867


    [1] 0.7970277 0.7912428 0.6586874 0.9864353 0.9855376 0.6572910 0.7717933
    [8] 0.6678636 0.7389787 0.7698983 0.6628765 0.7701975 0.7744863 0.9846399

As accuracy or sensitivity is increasing to a model, its specificity is decreasing. We discard models 4,5,14 as their accuracies are very low. We take average of predicted probabilities of remaining models. We use this averaged probability to get  output.


    predict_avg<-predict_values[,1]
    for(j in c(1,2,3,6,7,8,9,10,11,12,13)){
     predict_avg<-predict_avg+predict_values[,j]
      }
    predict_avg<-predict_avg/11
    predicted_values<-ifelse(predict_avg>0.5,1,0)
 
    conf_matrix<-table(train_data2[,2],predicted_values)
    accuracy_avg<-(conf_matrix[1,1]+conf_matrix[2,2])/sum(conf_matrix)
    accuracy_avg

Output

     0.78316
Average Specificity

    specificity_avg<-conf_matrix[2,2]/(conf_matrix[2,2]+conf_matrix[2,1])
    specificity_avg

Output
      0.778077

Average Sensitivity
  
    sensitivity_avg<-conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[1,2])
    sensitivity_avg

Output
    
     0.7835241

Specificity of the averaged model is 0.778 and accuracy is 0.783 which are good values. We can still increase the specificity by decreasing the threshold, but by doing so accuracy and sensitivity might decreas.

Let’s check specificity,accuracy for a decreased threshold value of 0.4.



    predicted_values<-ifelse(predict_avg>0.4,1,0)
 
    conf_matrix<-table(train_data2[,2],predicted_values)
    accuracy_avg<-(conf_matrix[1,1]+conf_matrix[2,2])/sum(conf_matrix)
    accuracy_avg

Output

    0.71504

Average Sensitivity

    sensitivity_avg<-conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[1,2])
    sensitivity_avg


Output
 
     0.7060811

Average Specificity

     specificity_avg<-conf_matrix[2,2]/(conf_matrix[2,2]+conf_matrix[2,1])
     specificity_avg
    
Output

      0.8401157


We can see Specificity has increased to 0.84 and but Accuracy of our Averaging model has decreased to 0.71.

Thus we have able to increase the Specificity which is given much more importance than compared to accuracy for these type of problems

>##END


